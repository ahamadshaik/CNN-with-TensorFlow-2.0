{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_TensorFlow2.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWN0NptYYIW8",
        "colab_type": "text"
      },
      "source": [
        "**CNN using Keras on MNIST dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8h_lxOdXem2",
        "colab_type": "text"
      },
      "source": [
        "- Importing necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_IuN3DlPhwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dggQdRSBXh9m",
        "colab_type": "text"
      },
      "source": [
        "- Architecture of the Network model\n",
        " - One pool layer set with 16 channels of 3*3 filters.\n",
        " - two poll layer sets with 32 channels of 3*3 filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MutngyCxPkUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\tmodel = Sequential([\n",
        "\t\tConv2D(16, (3, 3), padding=\"same\", input_shape=inputShape),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# (CONV => RELU => BN) * 2 => POOL layer set\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tConv2D(32, (3, 3), padding=\"same\"),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(axis=chanDim),\n",
        "\t\tMaxPooling2D(pool_size=(2, 2)),\n",
        "\t\t# first (and only) set of FC => RELU layers\n",
        "\t\tFlatten(),\n",
        "\t\tDense(256),\n",
        "\t\tActivation(\"relu\"),\n",
        "\t\tBatchNormalization(),\n",
        "\t\tDropout(0.5),def build_model(width, height, depth, classes):\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\t\t# softmax classifier\n",
        "\t\tDense(classes),\n",
        "\t\tActivation(\"softmax\")\n",
        "\t])\n",
        "\t# return the built model to the calling function\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDvGjauzXqKp",
        "colab_type": "text"
      },
      "source": [
        "- Optimization happens here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRCgZQgJP5jD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X, y):\n",
        "\t# keep track of our gradients\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\t# make a prediction using the model and then calculate the\n",
        "\t\t# loss\n",
        "\t\tpred = model(X)\n",
        "\t\tloss = categorical_crossentropy(y, pred)\n",
        "\t# calculate the gradients using our tape and then update the\n",
        "\t# model weights\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\topt.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4XCuAE5X6X3",
        "colab_type": "text"
      },
      "source": [
        "- Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oez7kHh2QNcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 3000\n",
        "lr = 0.01\n",
        "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
        "trainX = trainX.astype(\"float32\") / 255.0\n",
        "testX = testX.astype(\"float32\") / 255.0\n",
        "trainY = to_categorical(trainY, 10)\n",
        "testY = to_categorical(testY, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYvUcFiJQoHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(28, 28, 1, 10)\n",
        "opt = Adam(lr=lr, decay=lr / epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwnnz-9TRHrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = tf.data.Dataset.from_tensor_slices(\n",
        "    (trainX.reshape([-1, 28, 28, 1]).astype(np.float32) / 255, trainY.astype(np.int32)))\n",
        "data = data.shuffle(buffer_size=60000).batch(128).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqBTY2JOXtxo",
        "colab_type": "text"
      },
      "source": [
        "- Training Phase "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GCafbhTQ0h6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "54902690-c5f1-4d34-e2e9-b741e4b154c3"
      },
      "source": [
        "for step, (img_batch, lbl_batch) in enumerate(data):\n",
        "    if step > epochs:\n",
        "        break\n",
        "    train(img_batch, lbl_batch)\n",
        "    if not step % 100:\n",
        "        logits = model(img_batch)\n",
        "        loss = tf.keras.backend.mean(tf.keras.losses.categorical_crossentropy(lbl_batch, logits))\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, tf.argmax(lbl_batch, axis =1, output_type=tf.int32)),\n",
        "                             tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(loss, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.3243069648742676 Accuracy: 0.890625\n",
            "Loss: 0.14792966842651367 Accuracy: 0.9453125\n",
            "Loss: 0.010317481122910976 Accuracy: 1.0\n",
            "Loss: 0.060455046594142914 Accuracy: 0.9765625\n",
            "Loss: 0.028316056355834007 Accuracy: 1.0\n",
            "Loss: 0.027193576097488403 Accuracy: 0.9921875\n",
            "Loss: 0.07623466849327087 Accuracy: 0.984375\n",
            "Loss: 0.026266302913427353 Accuracy: 1.0\n",
            "Loss: 0.010583963245153427 Accuracy: 1.0\n",
            "Loss: 0.020456988364458084 Accuracy: 0.9921875\n",
            "Loss: 0.0022524348460137844 Accuracy: 1.0\n",
            "Loss: 0.005889228545129299 Accuracy: 1.0\n",
            "Loss: 0.0016094091115519404 Accuracy: 1.0\n",
            "Loss: 0.051517244428396225 Accuracy: 0.984375\n",
            "Loss: 0.006235375534743071 Accuracy: 1.0\n",
            "Loss: 0.008865944109857082 Accuracy: 1.0\n",
            "Loss: 0.004106742795556784 Accuracy: 1.0\n",
            "Loss: 0.006427858490496874 Accuracy: 1.0\n",
            "Loss: 0.0016038736794143915 Accuracy: 1.0\n",
            "Loss: 0.0073527805507183075 Accuracy: 1.0\n",
            "Loss: 0.0023270398378372192 Accuracy: 1.0\n",
            "Loss: 0.0032665079925209284 Accuracy: 1.0\n",
            "Loss: 0.001481209765188396 Accuracy: 1.0\n",
            "Loss: 0.0055723972618579865 Accuracy: 1.0\n",
            "Loss: 0.005143530201166868 Accuracy: 1.0\n",
            "Loss: 0.010812187567353249 Accuracy: 0.9921875\n",
            "Loss: 0.003438702318817377 Accuracy: 1.0\n",
            "Loss: 0.0020684320479631424 Accuracy: 1.0\n",
            "Loss: 0.012811699882149696 Accuracy: 1.0\n",
            "Loss: 0.0004002325586043298 Accuracy: 1.0\n",
            "Loss: 0.005535292439162731 Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMAlEjIRXYdY",
        "colab_type": "text"
      },
      "source": [
        "- Test Phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lee0aBb1V3pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images = testX.reshape([-1, 28, 28, 1])\n",
        "test_labels = testY.astype(np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T3DyETtWPTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd5d3924-495e-48f8-82df-2f231019d855"
      },
      "source": [
        "logits = model(test_images)\n",
        "preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(preds, tf.argmax(test_labels, axis =1, output_type=tf.int32)),\n",
        "                             tf.float32))\n",
        "print(\"Test Accuracy: {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9882000088691711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfhiJTLvo3RB",
        "colab_type": "text"
      },
      "source": [
        "- I have tried different set of hypermeters for learning rate and epochs.\n",
        "- I have tried different activation functions (tanh, sigmoid, relu and LeakyRelu)\n",
        "- I have used different loss functions and adam optimizers also."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpvu3tIIpiAN",
        "colab_type": "text"
      },
      "source": [
        "- As CIFAR-10 taking so-much time to train, I can't able to do experiments with it."
      ]
    }
  ]
}